
services:
  ollama:
    image: ollama/ollama:0.3.13
    container_name: vela_ollama
    restart: always
    ports:
      - "11434:11434"            # local API
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ../models/ollama:/root/.ollama   # models/cache on SSD

  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: vela_webui
    restart: always
    depends_on:
      - ollama
    ports:
      - "3000:8080"              # web UI on http://localhost:3000
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - ./data/open-webui:/app/backend/data
